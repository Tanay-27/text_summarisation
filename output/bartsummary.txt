BART stands for Bidirectional and Auto-Regressive Transformer. It is a denoising autoencoder that is a pre-trained sequence-to-sequence method, that uses masked language modeling for Natural Language Generation and Translation. BART architecture is similar to an encoder-decoder network except that it uses a combination of BERT and GPT models. The BART models can be fine-tuned over small supervised datasets to create domain-specific tasks. It was developed by Lewis et al. in 2019.
denoising autoencoder is trained on a large corpus of such data so it learns to predict the masked/deleted token in the input sentence which is responsible for the noise in the text. As a result, we get a clean and semantically coherent output, hence the term ‘denoise’ is added to the autoenCoder. For a given input text sequence, the BERT (Bidirectional Representation for Transformers) encoder network generates an embedding for each token.
BART’s primary task is used to generate clean semantically coherent text from corrupted text data. It can also be used for a variety of different NLP sub-tasks like language translation, question-answering tasks, text summarization, paraphrasing, etc. BART has approximately 140 million parameters which are greater than BERT (110 million parameters) and GPT-1 (117 million) models but outperform them significantly given that BART is a combination of them both. For its encoder model, BART uses a bi-directional encoder that is used in BERT, and for its decoder mode, it uses an autoregressive decoder.
summarize: nput tokens as well as the current token to predict the next token at every time step. It is important to remember that the input accepted by a decoder is an embedding created by its corresponding encoder network. Both the encoder and decoder architecture is built by the combination of multiple blocks or layers where each block processes information in a specific way. It consists of 3 primary blocks: Multi-head Attention block, Addition and Normalization block andFeed-forward layers.
The feed-forward layers compose the basic building block of any neural network and are composed of hidden layers containing a fixed number of neurons. These layers contain the process, and store information coming from the previous layers as weights and forward the processed/ updated information to the next layer. This is performed so that uniform weight for all parameters is ensured while concatenating multiple parameters into a single one. The feed- forward neural network layers are specially designed to move information in a sequential uni-directional manner. We use a monotonic function whose value converges to a constant value k as the input closes to infinity.
The tokenized text is then uni-label encoded and passed into the multi-head attention block where the text tokens are randomly masked and forwarded to the add and norm layer. We use a skip connection from the input layer to combine both the complete clean text as well as randomly masked tokens. After multiple such iterations, we then pass the information into the standard feed-forward block which also adds and normalizes the original information from the first “add and normalization” layer. The result is an embedding containing clean, masked, and compressed information regarding the original input text.
The GPT decoder cell accepts masked embeddings from the BERT Encoder cell and passes it to the masked multiple self-attention block. This is done so that information learned at the end of the sentence should also be able to adjust the weight of the embedding and cause a semantic change at the beginning of the output tokens if required. Following this, the output is added and normalized with the original embedding. It follows the same architecture as the multi-head attention block but works sequentially instead of parallel, where instead of learning the encoding of the masks, this layer learns to decode the masks.
The training process continues over a very large corpus of examples, that not only learn the context of the sentences but also greatly improve the network’s capability in predicting missing <MASK> tokens. This helps the network clean real-life corrupted sentences. At the final layer, we try to match the predicted tokens with the clean output which is the backtracked over the entire encoder-decoder network. It is hoped that this article will provide some insight into the development of the network.
